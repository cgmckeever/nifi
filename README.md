# Reference
Terraform with Ansible to create/manage a full AWS-based secure Apache NiFi cluster/stack.

# Requirements
- Terraform installed.
- AWS credentials (e.g. `aws configure` if awscli is installed)
- Customized variables (see Variables section).

# Variables
Edit the vars file (.tfvars) to customize the deployment, especially:
- bucket_name
 - a unique bucket name, terraform will create the bucket to store various resources.
- mgmt_cidr
 - an IP range granted NiFi webUI and EC2 SSH access via the ELB hostname.
 - deploying from home? `dig +short myip.opendns.com @resolver1.opendns.com | awk '{ print $1"/32" }'`
- kms_manager
 - an AWS user account (not root) that will be granted access to the KMS key (to read S3 objects).
- instance_key
 - a public SSH key for SSH access to instances.

# Deploy
```
# Initialize terraform
terraform init

# Apply terraform - the first apply takes a while creating an encrypted AMI.
terraform apply -var-file="tf-nifi.tfvars"

# Wait for SSM Ansible Playbook, watch:
https://console.aws.amazon.com/systems-manager/state-manager
```

# WebUI Access
WebUI access is permitted to the mgmt_cidr defined in tf-nifi.tfvars. Authentication requires the admin password-protected certificate generated by the Ansible playbook:
- Gather keystore.pkcs12 and tls.json from either:
  - The S3 bucket (defined in tf-nifi.tfvars)/admin-certificates/, or
  - An EC2 instance (via ssh to ELB hostname) under /mnt/tf-nifi-efs/admin-certificates/
- Import keystore.pkcs12 as certificate into Web Browser
  - Use tls.json's keyStorePassword value when prompted for password

# AMI Notes
- AMI is [Latest Official RHEL7](https://access.redhat.com/solutions/15356), but takes a considerable amount of time to clone. This method may change in the future, or Ubuntu may be used.
- The RedHat AMI [has this misconfiguration](https://bugzilla.redhat.com/show_bug.cgi?id=1865991).

# Ansible / SSM Notes
There are two Ansible playbooks deployed via terraform to AWS SSM, zookeepers/zookeepers.yml and nodes/nodes.yml.
- Both playbooks install Apache NiFi.
- Zookeepers includes Apache Zookeeper and custom services (mini playbooks) for cluster management.
- Playbooks are deployed to instances via SSM at instance launch time, though an administrator may reapply the SSM association at any time.

# Scaling Notes
Special actions take place during scaling to handle NiFi cluster management:
## Scale Up
- At node EC2 instance creation, the nodes.yml playbook installs NiFi and touches a file in the EFS mount dir cluster/join/.
- Zookeepers watch EFS:/cluster/join/, if join file(s) are found, a Zookeeper will copy up-to-date NiFi configurations to EFS:/conf/, remove the join file(s), and touch invite file(s).
- Nodes wait for the invite file(s), then copy the up-to-date EFS:/conf/ files to their local NiFi dir and start the NiFi service.

## Scale Down
- The Autoscaling Group has a Lifecycle Hook to an SNS topic.
- The SNS topic has a subscription from a Lambda function.
- The Lambda function executes an SSM RunCommand with an SSM Document.
- The SSM Document executes the node shell script (see: `nodes/scale-down`) on the terminating instances.
- `scale-down` perform several steps:
  - Retrieves the NodeId from the cluster.
  - Disconnects the NodeId from the cluster.
  - Offloads the NodeId from the cluster.
  - Touches a delete file in EFS:/cluster/leave/
  - Notifies AWS Autoscaling the lifecycle action is complete (and the instance may be terminated)
- Zookeepers watch EFS:/cluster/leave/, if leave file(s) are found, a Zookeeper will delete the matching NodeId from the cluster.
