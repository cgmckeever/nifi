# Reference
Terraform with Ansible to create/manage a full AWS-based secure Apache NiFi cluster/stack.

# Requirements
- Terraform installed.
- AWS credentials (e.g. `aws configure` if awscli is installed)
- Customized variables (see Variables section).

# Variables
Edit the vars file (.tfvars) to customize the deployment, especially:
**bucket_name**
- a unique bucket name, terraform will create the bucket to store various resources.
**mgmt_cidr**
- an IP range granted NiFi webUI and EC2 SSH access via the ELB hostname.
- deploying from home? `dig +short myip.opendns.com @resolver1.opendns.com | awk '{ print $1"/32" }'`
**kms_manager**
- an AWS user account (not root) that will be granted access to the KMS key (to read S3 objects).
**instance_key**
- a public SSH key for SSH access to instances.

# Deploy
```
# Initialize terraform
terraform init

# Apply terraform - the first apply takes a while creating an encrypted AMI.
terraform apply -var-file="tf-nifi.tfvars"

# Wait for SSM Ansible Playbook, watch:
https://console.aws.amazon.com/systems-manager/state-manager
```

# WebUI Access
WebUI access is permitted to the mgmt_cidr defined in tf-nifi.tfvars. Authentication requires the admin password-protected certificate generated by the Ansible playbook:
- Gather keystore.pkcs12 and tls.json from either:
  - The S3 bucket (defined in tf-nifi.tfvars)/admin-certificates/, or
  - An EC2 instance (via ssh to ELB hostname) under /mnt/tf-nifi-efs/admin-certificates/
- Import keystore.pkcs12 as certificate into Web Browser
  - Use tls.json's keyStorePassword value when prompted for password
- Browse to the zookeeper elb dns name, e.g.: `https://tf-nifi-zk-elb-123456.us-east-2.elb.amazonaws.com/nifi`

# AMI Notes
- AMI is [Ubuntu 1804](https://cloud-images.ubuntu.com/locator/ec2/), change the vendor_ami_name_string var as needed (especially the date).

# Ansible / SSM Notes
There are two Ansible playbooks deployed via terraform to AWS SSM, zookeepers/zookeepers.yml and nodes/nodes.yml.
- Both playbooks install Apache NiFi.
- Zookeepers includes Apache Zookeeper and custom services (mini playbooks) for cluster management.
- Playbooks are deployed to instances via SSM at instance launch time, though an administrator may reapply the SSM association at any time.

# Scaling Notes
Special actions take place during scaling to handle NiFi cluster management.
**Scale Up**
Every node spawned via Autoscale applies the nodes.yml playbook via SSM, which:
- installs pre-requisting packages/libraries and the Apache NiFi software
- touches a file in EFS:/mnt/nifi/cluster/join/<node name>
- a Zookeeper monitoring the join directory copies the latest NiFi conf to EFS:/mnt/nifi/cluster/conf/
- a Zookeeper touches files EFS:/mnt/nifi/cluster/invite/<node name>
- copies the up-to-date NiFi configuration from EFS:/mnt/nifi/cluster/conf/ to /opt/nifi/conf/
- starts the NiFi service, joining the cluster.

**Scale Down**
Every node terminated via Autoscale uses special actions to gracefully exit the cluster:
- Autoscale uses a Lifecycle Hook to notify an SNS topic of scale down.
- SNS topic has a subscription: a Lambda function.
- Lambda function spawns an SSM RunCommand.
- SSM RunCommand executes a shell script: `nodes/scale-down` on the terminating instance(s).
- `scale-down` actions include:
  - Retrieve instance's NodeId (node's ID within the NiFi cluster).
  - Disconnects from the NiFi cluster.
  - Offloads in-flight work from the cluster.
  - Touches a file in EFS:/cluster/leave/<node id>
  - a Zookeeper touches files EFS:/mnt/nifi/cluster/leave/ deletes the NodeId.
  - Completes the Autoscale Lifecycle Hook
  - AWS terminates the instance.
